{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI based Natural disaster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (1.19.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras==2.2.4) (5.4.1)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.35.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.11.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.35.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.19.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (52.0.0.post20211006)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4\n",
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np#used for numerical analysis\n",
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_8afb15d112f14e799e8fde1f1bb9dacd = 'https://s3.us.cloud-object-storage.appdomain.cloud'\n",
    "else:\n",
    "    endpoint_8afb15d112f14e799e8fde1f1bb9dacd = 'https://s3.private.us.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "client_8afb15d112f14e799e8fde1f1bb9dacd = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='diRTl2M3775Lk6XgwYxC1LZ5TWBvpeTTaRRT04VunvSC',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=endpoint_8afb15d112f14e799e8fde1f1bb9dacd)\n",
    "\n",
    "\n",
    "streaming_body_2 = client_8afb15d112f14e799e8fde1f1bb9dacd.get_object(Bucket='naturaldisasters-donotdelete-pr-rldfhdatayr1mr', Key='dataset.zip')['Body']\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "import tensorflow #open source used for both ML and DL for computation\n",
    "from tensorflow.keras.models import Sequential #it is a plain stack of layers\n",
    "from tensorflow.keras import layers #A layer consists of a tensor-in tensor-out computation function\n",
    "#Dense layer is the regular deeply connected neural network layer\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "#Faltten-used fot flattening the input or change the dimension\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D #Convolutional layer\n",
    "#MaxPooling2D-for downsampling the image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "unzip = zipfile.ZipFile(BytesIO(streaming_body_2.read()),'r')\n",
    "file_paths = unzip.namelist()\n",
    "for path in file_paths:\n",
    "    unzip.extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0-tf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.listdir('/home/wsuser/work/dataset/test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameter for Image Data agumentation to the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "#Image Data agumentation to the testing data\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data and performing data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 742 images belonging to 4 classes.\n",
      "Found 198 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#performing data agumentation to train data\n",
    "x_train = train_datagen.flow_from_directory('/home/wsuser/work/dataset/train_set',target_size=(64, 64),batch_size=5,\n",
    "                                            color_mode='rgb',class_mode='categorical')\n",
    "#performing data agumentation to test data\n",
    "x_test = test_datagen.flow_from_directory('/home/wsuser/work/dataset/test_set',target_size=(64, 64),batch_size=5,\n",
    "                                            color_mode='rgb',class_mode='categorical') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)#checking the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"
     ]
    }
   ],
   "source": [
    "print(x_test.class_indices)#checking the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 220, 1: 156, 2: 198, 3: 168})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter as c\n",
    "c(x_train .labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 10:56:45.540357: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ibm/dsdriver/lib:/opt/oracle/lib:/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/tensorflow\n",
      "2021-11-28 10:56:45.540410: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and poolingo\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dense(units=4, activation='softmax')) # softmax for more than 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               495744    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 524,900\n",
      "Trainable params: 524,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()#summary of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "# categorical_crossentropy for more than 2\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/wsuser/ipykernel_812/3995502952.py:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/40\n",
      "149/149 [==============================] - 34s 228ms/step - loss: 1.2206 - accuracy: 0.4367 - val_loss: 1.1049 - val_accuracy: 0.5505\n",
      "Epoch 2/40\n",
      "149/149 [==============================] - 33s 221ms/step - loss: 1.0992 - accuracy: 0.5054 - val_loss: 1.0189 - val_accuracy: 0.5657\n",
      "Epoch 3/40\n",
      "149/149 [==============================] - 32s 218ms/step - loss: 1.0572 - accuracy: 0.5526 - val_loss: 1.0673 - val_accuracy: 0.5556\n",
      "Epoch 4/40\n",
      "149/149 [==============================] - 33s 219ms/step - loss: 0.9359 - accuracy: 0.5876 - val_loss: 0.9340 - val_accuracy: 0.6212\n",
      "Epoch 5/40\n",
      "149/149 [==============================] - 33s 219ms/step - loss: 0.8087 - accuracy: 0.6307 - val_loss: 0.9783 - val_accuracy: 0.6364\n",
      "Epoch 6/40\n",
      "149/149 [==============================] - 32s 218ms/step - loss: 0.7887 - accuracy: 0.6765 - val_loss: 0.9030 - val_accuracy: 0.6212\n",
      "Epoch 7/40\n",
      "149/149 [==============================] - 33s 220ms/step - loss: 0.7223 - accuracy: 0.6941 - val_loss: 0.9866 - val_accuracy: 0.6364\n",
      "Epoch 8/40\n",
      "149/149 [==============================] - 33s 218ms/step - loss: 0.7080 - accuracy: 0.6927 - val_loss: 0.9329 - val_accuracy: 0.6212\n",
      "Epoch 9/40\n",
      "149/149 [==============================] - 33s 220ms/step - loss: 0.6801 - accuracy: 0.7116 - val_loss: 1.2452 - val_accuracy: 0.5354\n",
      "Epoch 10/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.6719 - accuracy: 0.7345 - val_loss: 0.8858 - val_accuracy: 0.6414\n",
      "Epoch 11/40\n",
      "149/149 [==============================] - 32s 216ms/step - loss: 0.5881 - accuracy: 0.7655 - val_loss: 1.0570 - val_accuracy: 0.5808\n",
      "Epoch 12/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.5672 - accuracy: 0.7655 - val_loss: 0.8223 - val_accuracy: 0.6869\n",
      "Epoch 13/40\n",
      "149/149 [==============================] - 32s 213ms/step - loss: 0.5179 - accuracy: 0.8046 - val_loss: 0.8120 - val_accuracy: 0.6919\n",
      "Epoch 14/40\n",
      "149/149 [==============================] - 33s 219ms/step - loss: 0.4513 - accuracy: 0.8329 - val_loss: 0.7991 - val_accuracy: 0.7071\n",
      "Epoch 15/40\n",
      "149/149 [==============================] - 32s 217ms/step - loss: 0.4493 - accuracy: 0.8100 - val_loss: 0.8811 - val_accuracy: 0.7222\n",
      "Epoch 16/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.4344 - accuracy: 0.8315 - val_loss: 0.8890 - val_accuracy: 0.7071\n",
      "Epoch 17/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.3748 - accuracy: 0.8491 - val_loss: 0.9510 - val_accuracy: 0.7222\n",
      "Epoch 18/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.3505 - accuracy: 0.8706 - val_loss: 1.1665 - val_accuracy: 0.6566\n",
      "Epoch 19/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.3120 - accuracy: 0.8801 - val_loss: 0.8183 - val_accuracy: 0.7424\n",
      "Epoch 20/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.2666 - accuracy: 0.9084 - val_loss: 1.0834 - val_accuracy: 0.7172\n",
      "Epoch 21/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.3223 - accuracy: 0.8774 - val_loss: 1.0538 - val_accuracy: 0.6717\n",
      "Epoch 22/40\n",
      "149/149 [==============================] - 32s 216ms/step - loss: 0.2698 - accuracy: 0.8935 - val_loss: 0.8276 - val_accuracy: 0.7576\n",
      "Epoch 23/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.2400 - accuracy: 0.8989 - val_loss: 0.9707 - val_accuracy: 0.7323\n",
      "Epoch 24/40\n",
      "149/149 [==============================] - 32s 217ms/step - loss: 0.2510 - accuracy: 0.8989 - val_loss: 1.1472 - val_accuracy: 0.7323\n",
      "Epoch 25/40\n",
      "149/149 [==============================] - 32s 213ms/step - loss: 0.2226 - accuracy: 0.9178 - val_loss: 1.2342 - val_accuracy: 0.6465\n",
      "Epoch 26/40\n",
      "149/149 [==============================] - 32s 212ms/step - loss: 0.2183 - accuracy: 0.9057 - val_loss: 1.5288 - val_accuracy: 0.7273\n",
      "Epoch 27/40\n",
      "149/149 [==============================] - 32s 217ms/step - loss: 0.3106 - accuracy: 0.8908 - val_loss: 0.8549 - val_accuracy: 0.7121\n",
      "Epoch 28/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.2608 - accuracy: 0.9043 - val_loss: 1.1995 - val_accuracy: 0.7020\n",
      "Epoch 29/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.1907 - accuracy: 0.9245 - val_loss: 1.3492 - val_accuracy: 0.6970\n",
      "Epoch 30/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.2063 - accuracy: 0.9259 - val_loss: 1.3965 - val_accuracy: 0.6667\n",
      "Epoch 31/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.1516 - accuracy: 0.9488 - val_loss: 1.3943 - val_accuracy: 0.6970\n",
      "Epoch 32/40\n",
      "149/149 [==============================] - 32s 216ms/step - loss: 0.1351 - accuracy: 0.9515 - val_loss: 1.6825 - val_accuracy: 0.6768\n",
      "Epoch 33/40\n",
      "149/149 [==============================] - 32s 212ms/step - loss: 0.1924 - accuracy: 0.9272 - val_loss: 1.5859 - val_accuracy: 0.7071\n",
      "Epoch 34/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.1326 - accuracy: 0.9623 - val_loss: 1.5799 - val_accuracy: 0.6919\n",
      "Epoch 35/40\n",
      "149/149 [==============================] - 32s 217ms/step - loss: 0.1594 - accuracy: 0.9420 - val_loss: 2.0303 - val_accuracy: 0.6616\n",
      "Epoch 36/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.1200 - accuracy: 0.9488 - val_loss: 1.5299 - val_accuracy: 0.7222\n",
      "Epoch 37/40\n",
      "149/149 [==============================] - 32s 212ms/step - loss: 0.1701 - accuracy: 0.9394 - val_loss: 1.6385 - val_accuracy: 0.7121\n",
      "Epoch 38/40\n",
      "149/149 [==============================] - 32s 214ms/step - loss: 0.1074 - accuracy: 0.9555 - val_loss: 1.7480 - val_accuracy: 0.6919\n",
      "Epoch 39/40\n",
      "149/149 [==============================] - 32s 215ms/step - loss: 0.1520 - accuracy: 0.9528 - val_loss: 1.4942 - val_accuracy: 0.6869\n",
      "Epoch 40/40\n",
      "149/149 [==============================] - 32s 212ms/step - loss: 0.0838 - accuracy: 0.9704 - val_loss: 1.7689 - val_accuracy: 0.6717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb556e8e070>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        generator=x_train,steps_per_epoch = len(x_train),\n",
    "        epochs=40, validation_data=x_test,validation_steps = len(x_test))# No of images in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "classifier.save('disaster_f.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = classifier.to_json()\n",
    "with open(\"model-bw.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11816\r\n",
      "drwxrwx--- 4 wsuser wscommon    4096 Nov 28 09:28 \u001b[0m\u001b[01;34mdataset\u001b[0m/\r\n",
      "-rw-rw---- 1 wsuser wscommon 6358984 Nov 28 11:50 disaster_f.h5\r\n",
      "-rw-rw---- 1 wsuser wscommon 5727832 Nov 28 12:00 image-classification-model_new.tgz\r\n",
      "-rw-rw---- 1 wsuser wscommon    3854 Nov 28 11:50 model-bw.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster_f.h5\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf image-classification-model_new.tgz disaster_f.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ibm_watson_machine_learning in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (1.0.173)\n",
      "Collecting ibm_watson_machine_learning\n",
      "  Downloading ibm_watson_machine_learning-1.0.175-py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lomond in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (0.3.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (3.10.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (1.26.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2.25.1)\n",
      "Requirement already satisfied: pandas<1.3.0,>=0.24.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (1.2.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (20.9)\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (0.8.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2021.10.8)\n",
      "Requirement already satisfied: ibm-cos-sdk==2.7.* in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm_watson_machine_learning) (2.7.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (2.7.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (2.7.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (0.15.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pandas<1.3.0,>=0.24.2->ibm_watson_machine_learning) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pandas<1.3.0,>=0.24.2->ibm_watson_machine_learning) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->ibm_watson_machine_learning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->ibm_watson_machine_learning) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from importlib-metadata->ibm_watson_machine_learning) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from packaging->ibm_watson_machine_learning) (2.4.7)\n",
      "Installing collected packages: ibm-watson-machine-learning\n",
      "  Attempting uninstall: ibm-watson-machine-learning\n",
      "    Found existing installation: ibm-watson-machine-learning 1.0.173\n",
      "    Uninstalling ibm-watson-machine-learning-1.0.173:\n",
      "      Successfully uninstalled ibm-watson-machine-learning-1.0.173\n",
      "Successfully installed ibm-watson-machine-learning-1.0.175\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm_watson_machine_learning --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "                   \"apikey\":\"KDjFaE3CaF3dFaKfFmlfkhI5rG5TAM6fWIqZOBKIWy92\"\n",
    "                  }\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guid_from_space_name(client, space_name):\n",
    "    space = client.spaces.get_details()\n",
    "    #print(space)\n",
    "    return(next(item for item in space['resources'] if item['entity'][\"name\"] == space_name)['metadata']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e11e4fae-ea39-4045-a975-1bbd7fb6758b\n"
     ]
    }
   ],
   "source": [
    "space_uid = guid_from_space_name(client, 'imageclassification')\n",
    "print(space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------------------------------  ----\n",
      "NAME                           ASSET_ID                              TYPE\n",
      "default_py3.6                  0062b8c9-8b7d-44a0-a9b9-46c416adcbd9  base\n",
      "pytorch-onnx_1.3-py3.7-edt     069ea134-3346-5748-b513-49120e15d288  base\n",
      "scikit-learn_0.20-py3.6        09c5a1d0-9c1e-4473-a344-eb7b665ff687  base\n",
      "spark-mllib_3.0-scala_2.12     09f4cff0-90a7-5899-b9ed-1ef348aebdee  base\n",
      "ai-function_0.1-py3.6          0cdb0f1e-5376-4f4d-92dd-da3b69aa9bda  base\n",
      "shiny-r3.6                     0e6e79df-875e-4f24-8ae9-62dcc2148306  base\n",
      "tensorflow_2.4-py3.7-horovod   1092590a-307d-563d-9b62-4eb7d64b3f22  base\n",
      "pytorch_1.1-py3.6              10ac12d6-6b30-4ccd-8392-3e922c096a92  base\n",
      "tensorflow_1.15-py3.6-ddl      111e41b3-de2d-5422-a4d6-bf776828c4b7  base\n",
      "scikit-learn_0.22-py3.6        154010fa-5b3b-4ac1-82af-4d5ee5abbc85  base\n",
      "default_r3.6                   1b70aec3-ab34-4b87-8aa0-a4a3c8296a36  base\n",
      "pytorch-onnx_1.3-py3.6         1bc6029a-cc97-56da-b8e0-39c3880dbbe7  base\n",
      "tensorflow_2.1-py3.6           1eb25b84-d6ed-5dde-b6a5-3fbdf1665666  base\n",
      "tensorflow_2.4-py3.8-horovod   217c16f6-178f-56bf-824a-b19f20564c49  base\n",
      "do_py3.8                       295addb5-9ef9-547e-9bf4-92ae3563e720  base\n",
      "autoai-ts_3.8-py3.8            2aa0c932-798f-5ae9-abd6-15e0c2402fb5  base\n",
      "tensorflow_1.15-py3.6          2b73a275-7cbf-420b-a912-eae7f436e0bc  base\n",
      "pytorch_1.2-py3.6              2c8ef57d-2687-4b7d-acce-01f94976dac1  base\n",
      "spark-mllib_2.3                2e51f700-bca0-4b0d-88dc-5c6791338875  base\n",
      "pytorch-onnx_1.1-py3.6-edt     32983cea-3f32-4400-8965-dde874a8d67e  base\n",
      "spark-mllib_3.0-py37           36507ebe-8770-55ba-ab2a-eafe787600e9  base\n",
      "spark-mllib_2.4                390d21f8-e58b-4fac-9c55-d7ceda621326  base\n",
      "xgboost_0.82-py3.6             39e31acd-5f30-41dc-ae44-60233c80306e  base\n",
      "pytorch-onnx_1.2-py3.6-edt     40589d0e-7019-4e28-8daa-fb03b6f4fe12  base\n",
      "default_r36py38                41c247d3-45f8-5a71-b065-8580229facf0  base\n",
      "autoai-obm_3.0                 42b92e18-d9ab-567f-988a-4240ba1ed5f7  base\n",
      "spark-mllib_2.4-r_3.6          49403dff-92e9-4c87-a3d7-a42d0021c095  base\n",
      "xgboost_0.90-py3.6             4ff8d6c2-1343-4c18-85e1-689c965304d3  base\n",
      "pytorch-onnx_1.1-py3.6         50f95b2a-bc16-43bb-bc94-b0bed208c60b  base\n",
      "autoai-ts_3.9-py3.8            52c57136-80fa-572e-8728-a5e7cbb42cde  base\n",
      "spark-mllib_2.4-scala_2.11     55a70f99-7320-4be5-9fb9-9edb5a443af5  base\n",
      "spark-mllib_3.0                5c1b0ca2-4977-5c2e-9439-ffd44ea8ffe9  base\n",
      "autoai-obm_2.0                 5c2e37fa-80b8-5e77-840f-d912469614ee  base\n",
      "spss-modeler_18.1              5c3cad7e-507f-4b2a-a9a3-ab53a21dee8b  base\n",
      "cuda-py3.8                     5d3232bf-c86b-5df4-a2cd-7bb870a1cd4e  base\n",
      "autoai-kb_3.1-py3.7            632d4b22-10aa-5180-88f0-f52dfb6444d7  base\n",
      "pytorch-onnx_1.7-py3.8         634d3cdc-b562-5bf9-a2d4-ea90a478456b  base\n",
      "spark-mllib_2.3-r_3.6          6586b9e3-ccd6-4f92-900f-0f8cb2bd6f0c  base\n",
      "tensorflow_2.4-py3.7           65e171d7-72d1-55d9-8ebb-f813d620c9bb  base\n",
      "spss-modeler_18.2              687eddc9-028a-4117-b9dd-e57b36f1efa5  base\n",
      "pytorch-onnx_1.2-py3.6         692a6a4d-2c4d-45ff-a1ed-b167ee55469a  base\n",
      "do_12.9                        75a3a4b0-6aa0-41b3-a618-48b1f56332a6  base\n",
      "spark-mllib_2.3-scala_2.11     7963efe5-bbec-417e-92cf-0574e21b4e8d  base\n",
      "spark-mllib_2.4-py37           7abc992b-b685-532b-a122-a396a3cdbaab  base\n",
      "caffe_1.0-py3.6                7bb3dbe2-da6e-4145-918d-b6d84aa93b6b  base\n",
      "pytorch-onnx_1.7-py3.7         812c6631-42b7-5613-982b-02098e6c909c  base\n",
      "cuda-py3.6                     82c79ece-4d12-40e6-8787-a7b9e0f62770  base\n",
      "tensorflow_1.15-py3.6-horovod  8964680e-d5e4-5bb8-919b-8342c6c0dfd8  base\n",
      "hybrid_0.1                     8c1a58c6-62b5-4dc4-987a-df751c2756b6  base\n",
      "pytorch-onnx_1.3-py3.7         8d5d8a87-a912-54cf-81ec-3914adaa988d  base\n",
      "-----------------------------  ------------------------------------  ----\n",
      "Note: Only first 50 records were displayed. To display more use 'limit' parameter.\n"
     ]
    }
   ],
   "source": [
    "client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab9e1b80-f2ce-592c-a7d2-4f2344f77194'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.8\")\n",
    "software_spec_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Warnings!! :  Model type keras_2.2.4 is deprecated. We recommend you use a supported model type. See Supported Frameworks https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html\n"
     ]
    }
   ],
   "source": [
    " model_details = client.repository.store_model(model=\"image-classification-model_new.tgz\",meta_props={\n",
    " client.repository.ModelMetaNames.NAME:\"CNN\",\n",
    " client.repository.ModelMetaNames.TYPE:\"keras_2.2.4\",\n",
    " client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_spec_uid }\n",
    "                                             )\n",
    "model_id = client.repository.get_model_uid(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da90b78d-7a6b-4fd6-bf9e-3d52f97cfa47'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "#model = load_model(\"disaster_f.h5\") #loading the model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/  disaster_f.h5  image-classification-model_new.tgz  model-bw.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/wsuser/work/dataset/test_set/Cyclone/867.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/wsuser/ipykernel_812/2119264984.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m img = image.load_img(\"/home/wsuser/work/dataset/test_set/Cyclone/867.jpeg\",grayscale=False,\n\u001b[0m\u001b[1;32m      2\u001b[0m                      target_size= (64,64))#loading of the image\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#image to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#changing the shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#predicting the classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/wsuser/work/dataset/test_set/Cyclone/867.jpeg'"
     ]
    }
   ],
   "source": [
    "img = image.load_img(\"/home/wsuser/work/dataset/test_set/Cyclone/867.jpeg\",grayscale=False,\n",
    "                     target_size= (64,64))#loading of the image\n",
    "x = image.img_to_array(img)#image to array\n",
    "x = np.expand_dims(x,axis = 0)#changing the shape\n",
    "pred = classifier.predict_classes(x)#predicting the classes\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['Cyclone','Earthquake','Flood','Wildfire']\n",
    "result=str(index[pred[0]])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterthemes as jt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t monokai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
